{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First step is to construct a ranking with a feature utility metric\n",
    "\n",
    "2) Choose a smaller set of the most useful features to develop initially "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of MI over correlation is that it can detect any kind of relationship, while correlation only detects linear relationships.\n",
    "\n",
    "Mutual information is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other.\n",
    "\n",
    "Technical note: what we're calling uncertainty is measured using a quantity from information theory known as \"entropy\". The entropy of a variable means roughly: \"how many yes-or-no questions you would need to describe an occurance of that variable, on average.\"\n",
    "\n",
    "If MI is zero, quantities are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to remember:\n",
    "\n",
    "    1) MI can help you to understand the relative potential of a feature as a predictor of the target\n",
    "    \n",
    "    2) It's possible for a feature to be very informative when interacting with other features, but not so informative \n",
    "       all alone. MI can't detect interactions between features. It is a univariate metric.\n",
    "       \n",
    "    3) The actual usefulness of a feature depends on the model you use it with. Just because a feature has a high \n",
    "       MI score doesn't mean your model will be able to do anything with that information. You may need to transform \n",
    "       the feature.\n",
    "       \n",
    "    4) The scikit-learn algorithm for MI treats discrete features differently from continuous features. \n",
    "       Consequently, you need to tell it which are which. Scikit-learn has two mutual information metrics \n",
    "       in it feature_selection module:\n",
    "            \n",
    "            a) one for real-valued targets (mutual_info_regression) \n",
    "\n",
    "            b) one for categorical targets (mutual_info_classif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"price\")\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "    \n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_utility_scores(scores):\n",
    "    y = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(y))\n",
    "    ticks = list(y.index)\n",
    "    plt.barh(width, y)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_utility_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we might expect, the high-scoring curb_weight feature exhibits a strong relationship with price, the target.\n",
    "\n",
    "sns.relplot(x=\"curb_weight\", y=\"price\", data=df);\n",
    "\n",
    "# The fuel_type feature has a fairly low MI score, but as we can see from the figure, \n",
    "# it clearly separates two price populations with different trends within the horsepower feature. \n",
    "# This indicates that fuel_type contributes an interaction effect and might not be unimportant after all. \n",
    "# Before deciding a feature is unimportant from its MI score, it's good to investigate any possible interaction effects \n",
    "# Domain knowledge can offer a lot of guidance here.\n",
    "\n",
    "sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
