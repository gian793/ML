{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Pandas & Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation and operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter notebook --notebook-dir=C:\\MyFolder\\Local_Docs\\Trainings\\MachineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "print('_'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index) # Drop all indexes used in df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset as response variable and feature variabes\n",
    "X = wine.drop('quality', axis = 1)\n",
    "y = wine['quality']\n",
    "\n",
    "# ... OR\n",
    "X = wine.copy()\n",
    "y = X.pop(\"price\")\n",
    "\n",
    "#Train and Test splitting of data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add/remove a column\n",
    "df['new'] = np.random.random(5)\n",
    "df.drop('new', axis=1, inplace=True)\n",
    "\n",
    "# Add/remove rows\n",
    "df.loc[5,:] = ['Jack', 3, 3, 4, 5, 1] # Sixth row\n",
    "df.drop(5, axis=0, inplace=True)\n",
    "\n",
    "# Insert a new column in a specified position\n",
    "df.insert(0, 'new', np.random.random(5) # New row in a specified position (i.e. position 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "my_list = []\n",
    "\n",
    "# Create a list from existing data\n",
    "col = list(train_data.columns)\n",
    "\n",
    "# Dictionary\n",
    "my_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015 Sales    30\n",
       "2016 Sales    35\n",
       "2017 Sales    40\n",
       "Name: Product A, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Series with a timestamp index (launched)\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "\n",
    "# Example \n",
    "d = {'a': 1, 'b': 2, 'c': 3}\n",
    "ser = pd.Series(data=d, index=['a', 'b', 'c'])\n",
    "\n",
    ">>> ser\n",
    "a   1\n",
    "b   2\n",
    "c   3\n",
    "dtype: int64\n",
    "# Example end\n",
    "\n",
    "# Creates a rolling window that contains all the data in the previous 7 days and counts the projects\n",
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "# Adjust the index so we can join it with the other training data\n",
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "# Join\n",
    "baseline_data.join(count_7_days)\n",
    "\n",
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600  # Difference with previous row\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "\n",
    "# Final time since last project\n",
    "timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\n",
    "\n",
    "# Create series\n",
    "dataser = pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')\n",
    "dataser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bob</th>\n",
       "      <th>Sue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Product A</th>\n",
       "      <td>I liked it.</td>\n",
       "      <td>Pretty good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product B</th>\n",
       "      <td>It was awful.</td>\n",
       "      <td>Bland.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Bob           Sue\n",
       "Product A    I liked it.  Pretty good.\n",
       "Product B  It was awful.        Bland."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data-frame\n",
    "datafr = pd.DataFrame({   'Bob': ['I liked it.', 'It was awful.'], \n",
    "                          'Sue': ['Pretty good.', 'Bland.']},\n",
    "                           index=['Product A', 'Product B'])\n",
    "\n",
    "datafr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indexes\n",
    "indices = np.where([earthquakes.Date.str.len() > 24])[1] # Index of 1st column where length of date > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product A      I liked it.\n",
       "Product B    It was awful.\n",
       "Name: Bob, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing - Equivalent forms\n",
    "\n",
    "#datafr['Bob']\n",
    "#datafr.iloc[:,0]\n",
    "#datafr.loc[:, 'Bob']\n",
    "datafr.Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc is primarily label based indexing. Integers may be used but they are interpreted as a label.\n",
    "# iloc is primarily integer based indexing\n",
    "# To select a subset of rows and columns from our DataFrame, we can use the iloc method. For example, we can select month, day and year (columns 2, 3 and 4 if we start counting at 1), like this:\n",
    "\n",
    "# iloc[row slicing, column slicing]\n",
    "surveys_df.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make \"title\" column the index of the data frame\n",
    "import pandas as pd\n",
    "reviews.set_index(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning\n",
    "top_oceania_wines = reviews.loc[reviews.country.isin(['Australia', 'New Zealand']) & (reviews.points >= 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas methods\n",
    "reviews.points.describe()\n",
    "reviews.points.mean()\n",
    "reviews.taster_name.unique()\n",
    "reviews.taster_name.value_counts()\n",
    "\n",
    "# Index max of ratio points/price\n",
    "bargain_wine = reviews.title[(reviews.points/reviews.price).idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Class-Mean(Survived)\n",
    "train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "\n",
    "\n",
    "  |Pclass|Survived|\n",
    "-------------------\n",
    "0    1    0.629630\n",
    "1    2    0.472826\n",
    "2    3    0.242363\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns names\n",
    "X_data.columns\n",
    "\n",
    "# Selecting a column\n",
    "y = X_data.Price\n",
    "\n",
    "#Selecting more columns\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "X = X_data[melbourne_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe avoiding original data changes\n",
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "\n",
    "# Models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "my_model = DecisionTreeRegressor(max_leaf_nodes=max_n_leaf, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "my_model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absoute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(val_y, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=500, learning_rate=0.05, n_jobs=4) # n_estimators [100-1000], n_jobs = n of PC cores\n",
    "\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5,             # stops after 5 deteriorating rounds\n",
    "             eval_set=[(X_valid, y_valid)],       # validation score data\n",
    "             verbose=False)\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all missing values\n",
    "dataset.replace('?', nan, inplace=True)\n",
    "\n",
    "# Remove all the rows that contain a missing value\n",
    "data_cleaned = nfl_data.dropna()\n",
    "\n",
    "# Remove all columns with at least one missing value\n",
    "data_cleaned = nfl_data.dropna(axis=1)\n",
    "\n",
    "# Replace all NA's with 0\n",
    "subset_nfl_data.fillna(0)\n",
    "\n",
    "# Replace all NA's the value that comes directly after it in the same column, then replace all the remaining na's with 0\n",
    "subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)\n",
    "\n",
    "# Fill missing values with the value of the previous day\n",
    "def fill_missing(df):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            if isnan(df[row, col]):\n",
    "                df[row, col] = df[row - one_day, col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target ('SaleProce'), separate target from predictors\n",
    "X1.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "# Or\n",
    "# Remove live projects rows\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Copy target\n",
    "y = X1.SalePrice\n",
    "\n",
    "# Drop 'SalePrice' column\n",
    "X1.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numerical (categorical) predictors\n",
    "X2 = X1.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns with missing values > 0\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing values columns name\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop missing values columns\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))  # Calculates mean and fits. New data frame has no column names\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))      # Fits using previously calculated mean\n",
    "\n",
    "# Include column names\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing feature with median\n",
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables columns\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "# .. alternatively\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# .. alternatively + low cardinality condition\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if\n",
    "                  X_train_full[cname].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numerical predictors\n",
    "X1 = X_train.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in object_cols:\n",
    "    label_X_train[object_cols] = label_encoder.fit_transform(X_train[object_cols])\n",
    "    label_X_valid[object_cols] = label_encoder.transform(X_valid[object_cols])\n",
    "    \n",
    "\n",
    "# ... other example\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "\n",
    "# Since ks and encoded have the same index and I can easily join them\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)  \n",
    "\n",
    "# ... or with column composition\n",
    "interactions = ks['category'] + \"_\" + ks['country'] # Poetry_GB\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "data_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... OR\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "    \n",
    "    \n",
    "# Factorize - Example\n",
    "codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])\n",
    "\n",
    ">>> codes\n",
    "array([0, 0, 1, 2, 0]...)\n",
    "\n",
    ">>> uniques\n",
    "array(['b', 'a', 'c'], dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols=cat_features)\n",
    "\n",
    "# Learn encoding from the training set\n",
    "count_enc.fit(train[cat_features])\n",
    "\n",
    "# Apply encoding to the train and validation sets\n",
    "train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))\n",
    "valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Target encoding replaces a categorical value with the average value of the target for that value of the feature.\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])     # -> only train data used to avoid leakage\n",
    "\n",
    "# Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "train_TE = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid_TE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Similar to target encoding in that it's based on the target probablity for a given value. \n",
    "# However with CatBoost, for each row, the target probability is calculated only from the rows before it.\n",
    "target_enc = ce.CatBoostEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])  # -> only train data used to avoid leakage\n",
    "\n",
    "# Transform the features, rename columns with _cb suffix, and join to dataframe\n",
    "train_CBE = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid_CBE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like \"label Encoding\". This is done by specifying explicitly the encoding i.e. {'female': 1, 'male': 0} on single columns\n",
    "dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded columns with numerical ones\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. or one-hot encoding with Pandas\n",
    "\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1) # X_train aligned according to X_valid\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)   # X_train aligned according to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad categorical columns\n",
    "\n",
    "# Collect categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low cardinality columns\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean') # Your code here\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')), # For categorical data with 'costant' -> default is “missing_value”\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0) # Your code here\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),  # \"make_pipeline\" generates names for steps automatically\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat features\n",
    "import itertools\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "interactions = pd.DataFrame(index=clicks.index)\n",
    "\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "\n",
    "    # Convert to strings and combine\n",
    "    new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str)\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    interactions[new_col_name] = encoder.fit_transform(new_values)\n",
    "\n",
    "clicks = clicks.join(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(train[feature_cols], train['outcome'])\n",
    "\n",
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), index=train.index, columns=feature_cols)\n",
    "\n",
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate methods consider only one feature at a time when making a selection decision. \n",
    "# Instead, we can make our selection using all of the features by including them in a linear model \n",
    "# with L1 regularization.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "X, y = train[train.columns.drop(\"outcome\")], train['outcome']\n",
    "\n",
    "# Set the regularization parameter C=1\n",
    "logistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X)\n",
    "\n",
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), index=X.index, columns=X.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "# Feature selection with L1 regularization is more powerful the univariate tests, but it can also be \n",
    "# very slow when you have a lot of data and a lot of features. Univariate tests will be much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,random_state=0))  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error') # cv -> number of folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split data\n",
    "valid_fraction = 0.1\n",
    "clicks_srt = clicks.sort_values('click_time')\n",
    "valid_rows = int(len(clicks_srt) * valid_fraction)\n",
    "train = clicks_srt[:-valid_rows * 2]\n",
    "\n",
    "# valid size == test size, last two sections of the data\n",
    "valid = clicks_srt[-valid_rows * 2:-valid_rows]\n",
    "test = clicks_srt[-valid_rows:]\n",
    "\n",
    "# Train data\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)\n",
    "\n",
    "# Predict on test set\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['is_attributed'], ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ser.fillna(value=ser.mode()[0])\n",
    "print('Pandas Time Elapsed:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation\n",
    "df_train['SalePrice'] = np.log(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(list('abca'))\n",
    "pd.get_dummies(s)\n",
    "\n",
    "#    a  b  c\n",
    "# 0  1  0  0\n",
    "# 1  0  1  0\n",
    "# 2  0  0  1\n",
    "# 3  1  0  0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellanea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B   C   D\n",
      "0  0  1   2   3\n",
      "1  4  5   6   7\n",
      "2  8  9  10  11\n",
      "---------\n",
      "A    4.0\n",
      "B    5.0\n",
      "C    6.0\n",
      "D    7.0\n",
      "dtype: float64\n",
      "---------\n",
      "   B   C   D\n",
      "0  1   2   3\n",
      "1  5   6   7\n",
      "2  9  10  11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A', 'B', 'C', 'D'])\n",
    "print(df)\n",
    "\n",
    "# Axis \n",
    "print('---------')\n",
    "print(df.mean(axis=1)) # 0: along rows (picking up a column at time), 1: along columns (picking up a row at time)\n",
    "print('---------')\n",
    "\n",
    "df = df.drop('A', axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Title\n",
    "dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Replace\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Dona'], 'Rare')\n",
    "    \n",
    "# Print survived mean grouped by title \n",
    "train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n",
    "\n",
    "# Title mapping\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)              # fill na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Age' bands\n",
    "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)  # -> cut: evenly spaced intervals, qcut: same qvalues (same number of occurrences)\n",
    "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'IsAlone' column\n",
    "for dataset in combine:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "# Create 'Age*Class' column\n",
    "for dataset in combine:\n",
    "    dataset['Age*Class'] = dataset.Age * dataset.Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to daily\n",
    "daily_groups = dataset.resample('D')\n",
    "daily_data = daily_groups.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Learn', 'Python', 'Challenge', 'Casino.']\n",
      "[4]\n",
      "[]\n",
      "['They', 'bought', 'a', 'car', 'and', 'a', 'casino']\n",
      "[6]\n",
      "[0]\n",
      "['Casinoville']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc_list = [\"The Learn Python Challenge Casino.\", \"They bought a car and a casino\", \"Casinoville\"]\n",
    "\n",
    "keywords = ['casino', 'they']\n",
    "\n",
    "def word_search(documents, keyword):\n",
    "    # list to hold the indices of matching documents\n",
    "    indices = [] \n",
    "    \n",
    "    # Iterate through the indices (i) and elements (doc) of documents\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Split the string doc into a list of words (according to whitespace)\n",
    "        tokens = doc.split()\n",
    "        \n",
    "        # Make a transformed list where we 'normalize' each word to facilitate matching.\n",
    "        # Periods and commas are removed from the end of each word, and it's set to all lowercase.\n",
    "        normalized = [token.rstrip('.,').lower() for token in tokens]\n",
    "        \n",
    "        # Is there a match? If so, update the list of matching indices.\n",
    "        if keyword.lower() in normalized:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "\n",
    "dct = {}\n",
    "\n",
    "for i, doc in enumerate(doc_list):\n",
    "    doc = doc.split()\n",
    "    print(doc)\n",
    "    for word in keywords:\n",
    "        index = word_search(doc, word)\n",
    "        dct[word] = index\n",
    "        print(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolls as Numpy ndarray [4 1 5 2 4 3 2 4 3 4]\n",
      "Rolls as list [4, 1, 5, 2, 4, 3, 2, 4, 3, 4]\n",
      "xlist as list [[1, 2, 3], [2, 4, 6]]\n",
      "xlist as ndarray\n",
      " [[1 2 3]\n",
      " [2 4 6]]\n",
      "[[1 2 3]\n",
      " [2 4 6]]\n",
      "Help on function ravel in module numpy:\n",
      "\n",
      "ravel(a, order='C')\n",
      "    Return a contiguous flattened array.\n",
      "    \n",
      "    A 1-D array, containing the elements of the input, is returned.  A copy is\n",
      "    made only if needed.\n",
      "    \n",
      "    As of NumPy 1.10, the returned array will have the same type as the input\n",
      "    array. (for example, a masked array will be returned for a masked array\n",
      "    input)\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.  The elements in `a` are read in the order specified by\n",
      "        `order`, and packed as a 1-D array.\n",
      "    order : {'C','F', 'A', 'K'}, optional\n",
      "    \n",
      "        The elements of `a` are read using this index order. 'C' means\n",
      "        to index the elements in row-major, C-style order,\n",
      "        with the last axis index changing fastest, back to the first\n",
      "        axis index changing slowest.  'F' means to index the elements\n",
      "        in column-major, Fortran-style order, with the\n",
      "        first index changing fastest, and the last index changing\n",
      "        slowest. Note that the 'C' and 'F' options take no account of\n",
      "        the memory layout of the underlying array, and only refer to\n",
      "        the order of axis indexing.  'A' means to read the elements in\n",
      "        Fortran-like index order if `a` is Fortran *contiguous* in\n",
      "        memory, C-like order otherwise.  'K' means to read the\n",
      "        elements in the order they occur in memory, except for\n",
      "        reversing the data when strides are negative.  By default, 'C'\n",
      "        index order is used.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    y : array_like\n",
      "        y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n",
      "        Note that matrices are special cased for backward compatibility, if `a`\n",
      "        is a matrix, then y is a 1-D ndarray.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.flat : 1-D iterator over an array.\n",
      "    ndarray.flatten : 1-D array copy of the elements of an array\n",
      "                      in row-major order.\n",
      "    ndarray.reshape : Change the shape of an array without changing its data.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In row-major, C-style order, in two dimensions, the row index\n",
      "    varies the slowest, and the column index the quickest.  This can\n",
      "    be generalized to multiple dimensions, where row-major order\n",
      "    implies that the index along the first axis varies slowest, and\n",
      "    the index along the last quickest.  The opposite holds for\n",
      "    column-major, Fortran-style index ordering.\n",
      "    \n",
      "    When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n",
      "    may be preferable.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    It is equivalent to ``reshape(-1, order=order)``.\n",
      "    \n",
      "    >>> x = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> np.ravel(x)\n",
      "    array([1, 2, 3, 4, 5, 6])\n",
      "    \n",
      "    >>> x.reshape(-1)\n",
      "    array([1, 2, 3, 4, 5, 6])\n",
      "    \n",
      "    >>> np.ravel(x, order='F')\n",
      "    array([1, 4, 2, 5, 3, 6])\n",
      "    \n",
      "    When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n",
      "    \n",
      "    >>> np.ravel(x.T)\n",
      "    array([1, 4, 2, 5, 3, 6])\n",
      "    >>> np.ravel(x.T, order='A')\n",
      "    array([1, 2, 3, 4, 5, 6])\n",
      "    \n",
      "    When ``order`` is 'K', it will preserve orderings that are neither 'C'\n",
      "    nor 'F', but won't reverse axes:\n",
      "    \n",
      "    >>> a = np.arange(3)[::-1]; a\n",
      "    array([2, 1, 0])\n",
      "    >>> a.ravel(order='C')\n",
      "    array([2, 1, 0])\n",
      "    >>> a.ravel(order='K')\n",
      "    array([2, 1, 0])\n",
      "    \n",
      "    >>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n",
      "    array([[[ 0,  2,  4],\n",
      "            [ 1,  3,  5]],\n",
      "           [[ 6,  8, 10],\n",
      "            [ 7,  9, 11]]])\n",
      "    >>> a.ravel(order='C')\n",
      "    array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n",
      "    >>> a.ravel(order='K')\n",
      "    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rolls = np.random.randint(low=1, high=6, size=10)\n",
    "print(\"Rolls as Numpy ndarray\", rolls)\n",
    "\n",
    "rolls_list = rolls.tolist()\n",
    "\n",
    "print(\"Rolls as list\",rolls_list)\n",
    "\n",
    "xlist = [[1,2,3],[2,4,6],]\n",
    "print(\"xlist as list\",xlist)\n",
    "# Create a 2-dimensional array\n",
    "x = np.asarray(xlist)\n",
    "print(\"xlist as ndarray\\n\",x)\n",
    "\n",
    "print(x)\n",
    "\n",
    "help(np.ravel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = '0'\n",
    "x1 = int(x) + 2\n",
    "\n",
    "print(x1)\n",
    "\n",
    "s = str(x1)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cells\n",
    "np.product(nfl_data.shape)\n",
    "\n",
    "# Total missing values\n",
    "nfl_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "\n",
    "# Assign time stamp columns -> See .dt (day-time) attribute\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map \n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points.map(lambda p: p - review_points_mean)\n",
    "\n",
    "# OR\n",
    "\n",
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row\n",
    "\n",
    "reviews.apply(remean_points, axis='columns')\n",
    "\n",
    "############################################à\n",
    "\n",
    "# Example: counts the number of words 'tropical', 'fruits' in description\n",
    "cnt_tropical = reviews.description.map(lambda desc: \"tropical\" in desc).sum()\n",
    "cnt_fruity   = reviews.description.map(lambda desc: \"fruity\" in desc).sum()\n",
    "\n",
    "descriptor_counts = pd.Series([cnt_tropical, cnt_fruity], index=['tropical', 'fruity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by groups\n",
    "reviews.groupby('points').points.count()\n",
    "\n",
    "# Min\n",
    "reviews.groupby('points').price.min()\n",
    "\n",
    "# Best wine by country and province\n",
    "reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])\n",
    "\n",
    "# Grouping with multiple functions\n",
    "reviews.groupby(['country']).price.agg([len, min, max])\n",
    "\n",
    "# To convert back to regular index \n",
    "countries_reviewed.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.sort_values(by='len', ascending=False)\n",
    "\n",
    "# By more than 1 column\n",
    "reviews.sort_values(by=['country', 'len'])\n",
    "reviews.groupby(['country', 'variety']).size().sort_values(ascending=False)\n",
    "\n",
    "# By index\n",
    "countries_reviewed.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert\n",
    "reviews.points.astype('float64')\n",
    "\n",
    "# To get all NaN countries\n",
    "reviews[pd.isnull(reviews.country)]\n",
    "\n",
    "# To fill NaN\n",
    "reviews.region_2.fillna(\"Unknown\")\n",
    "\n",
    "# To replace firstname with secondname\n",
    "reviews.taster_twitter_handle.replace(\"@firstname\", \"@secondname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename columns\n",
    "reviews.rename(columns={'region_1': 'region', 'region_2': 'locale'})\n",
    "reviews.rename(columns=dict(region_1='region', region_2='locale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename rows\n",
    "reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename row and colmun indexes \n",
    "reviews.rename_axis(\"wines\", axis='rows').rename_axis(\"fields\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat puts together DataFrame or Series with same fields\n",
    "canadian_youtube = pd.read_csv(\"../input/youtube-new/CAvideos.csv\")\n",
    "british_youtube = pd.read_csv(\"../input/youtube-new/GBvideos.csv\")\n",
    "\n",
    "pd.concat([canadian_youtube, british_youtube])\n",
    "\n",
    "# ... OR\n",
    "pd.concat([df, df2], axis=0, ignore_index=True) # To combine along columns, the axis parameter is set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dataFrame objects which have an index in common\n",
    "left = canadian_youtube.set_index(['title', 'trending_date'])\n",
    "right = british_youtube.set_index(['title', 'trending_date'])\n",
    "\n",
    "left.join(right, lsuffix='_CAN', rsuffix='_UK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge combines dataframes based on common values in a given column or columns\n",
    "customer.merge(order, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pivot_table transforms a dataframe to a format that explains the relationship among variables\n",
    "df.pivot_table(index='name', columns='ctg', aggfunc='mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "original_data = np.random.exponential(size=1000) # Data must be > 0 with BoxCox\n",
    "\n",
    "# Mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])\n",
    "\n",
    "# Normalize your data if you're going to be using a machine learning or statistics technique that assumes \n",
    "# your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and \n",
    "# Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "# Normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Get the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Endcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol: €\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "after = before.encode(\"utf-8\", errors=\"replace\")\n",
    "type(before)\n",
    "type(after)\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/ks-projects-201801.csv\", 'rb') as rawdata: result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "print(result) # Gives: {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n",
    "\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lower case\n",
    "professors['Country'] = professors['Country'].str.lower()\n",
    "\n",
    "# Remove trailing white spaces\n",
    "professors['Country'] = professors['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# Get the top 10 closest matches to \"south korea\"\n",
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[ 1. 18.]\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "MinMaxScaler()\n",
    "print(scaler.data_max_)\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the mean and scaling to unit variance to get otimized results\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
